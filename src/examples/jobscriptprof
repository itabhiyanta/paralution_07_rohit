#!/bin/bash
#$ -pe openmpi 1 
#$ -l h_rt=2:15:00
#$ -l fat,gpu=K20
#$ -N pdpcg_ongpu_guus1e7_16_meiluj_1e6_prof
#$ -cwd

APP=./dpcg_precon
#A x b phi openMP_threads xdim ydim zdim lssd_vex_flag defvex_x defvex_y defvex_z levelset_ghost_points
# ARGS="../../../dimitar_tsts_dec2013/AC128_1bub_sd.mtx  ../../../dimitar_tsts_dec2013/x_readin128_1bub_sd.rec ../../../dimitar_tsts_dec2013/b_readin128_1bub_sd.rec"
#ARGS="../../../convert_coo_to_mm/atry1_5.mtx ../../../convert_coo_to_mm/Z_5.mtx ../../../convert_coo_to_mm/r_5.dat 8"
#ARGS="/var/scratch/rgupta/dimitar_tsts_dec2013/AC128_9bub_lssd.mtx  /var/scratch/rgupta/dimitar_tsts_dec2013/x_readin128_9bub_lssd.rec /var/scratch/rgupta/dimitar_tsts_dec2013/b_readin128_9bub_lssd.rec /var/scratch/rgupta/dimitar_tsts_dec2013/phi_readin128_9bub_lssd.rec 128 1 2 2"
ARGS="/var/scratch/rgupta/guus_data/a_shell_norm1e7_16.mtx /var/scratch/rgupta/guus_data/Z_shell_norm1e7_16.mtx /var/scratch/rgupta/guus_data/r_shell_nrm1e7_16.dat /var/scratch/rgupta/guus_data/sol_shell_nrm1e7_16.dat 8"
# Get OpenMPI settings
. /etc/bashrc
module load intel/compiler/64
module load openmpi/intel/64
module load intel/mkl/64
module load cuda55/toolkit cuda55/blas cuda55/profiler
# Make new hostfile specifying the cores per node wanted
ncores=1
HOSTFILE=$TMPDIR/hosts
for host in `uniq $TMPDIR/machines`; do
    echo $host slots=$ncores
    done > $HOSTFILE
    nhosts=`wc -l < $HOSTFILE`
    totcores=`expr $nhosts \* $ncores`

# Use regular ssh-based startup instead of OpenMPI/SGE native one
unset PE_HOSTFILE
PATH=/usr/bin:$PATH
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/ROHIT/babysteps/paralution-0.5.0/build/lib
$MPI_RUN -np $totcores --hostfile $HOSTFILE nvprof-script $APP $ARGS
